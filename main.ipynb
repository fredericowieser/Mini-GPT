{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a ChatGPT Clone\n",
    "- GPT is a probabilistic system and so can give different answers.\n",
    "- Based on the famous \"Attention Is All You Need\" Transformer model.\n",
    "- G.P.T. stands for Generally Pretrained Transformer.\n",
    "- We will obviously not be building something even close to being as good as OpenAI's ChatGPT (OC).\n",
    "\n",
    "In this case though we are going to be trying to create a model to predict the next character and not the next word (in the case of OC).\n",
    "\n",
    "We will be training on the \"Tiny Shakespeare Dataset\" which is simply a text file containing all the literary works of Shakespeare.\n",
    "\n",
    "We will essentially be modelling how these characters evolve one fter another in a sequence.\n",
    "\n",
    "We will be creating a Language Model to output language that looks like Shakespeare.\n",
    "\n",
    "- https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4828s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator object at 0x10eebbdd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# RNG seed for reproducability\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str\n",
    "with open('tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read() # Store whole file in 1 string ~ 1 million characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "vocab_size = len(unique_chars)\n",
    "print(''.join(unique_chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google uses Sentence Piece\n",
    "# OpenAI uses Tik Token\n",
    "\n",
    "str_to_int = { ch:i for i, ch in enumerate(unique_chars) }\n",
    "int_to_str = { i:ch for i, ch in enumerate(unique_chars) }\n",
    "\n",
    "# Encoder and Decoder\n",
    "# Here we assign each character an integer. In prod we might\n",
    "# change this to be mutli-character.\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_str[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode tiny-shalespeare.txt dataset into PyTorch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching Our Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE = 8\n",
    "train_data[:BLOCK_SIZE+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If INPUT = tensor([18]) ; TARGET = 47\n",
      "If INPUT = tensor([18, 47]) ; TARGET = 56\n",
      "If INPUT = tensor([18, 47, 56]) ; TARGET = 57\n",
      "If INPUT = tensor([18, 47, 56, 57]) ; TARGET = 58\n",
      "If INPUT = tensor([18, 47, 56, 57, 58]) ; TARGET = 1\n",
      "If INPUT = tensor([18, 47, 56, 57, 58,  1]) ; TARGET = 15\n",
      "If INPUT = tensor([18, 47, 56, 57, 58,  1, 15]) ; TARGET = 47\n",
      "If INPUT = tensor([18, 47, 56, 57, 58,  1, 15, 47]) ; TARGET = 58\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:BLOCK_SIZE]\n",
    "Y = train_data[1:BLOCK_SIZE+1]\n",
    "for t in range(BLOCK_SIZE):\n",
    "    context = X[:t+1]\n",
    "    target = Y[t]\n",
    "    print(f\"If INPUT = {context} ; TARGET = {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the information we gaing above about how our model takes a sequence to then learns to predict the next element in that sequence. We split up this task by using a block size which limits the sequence size we are aiming to learn. In the following code we will take this concept of using a block size and also encoporate the idea of batching. Batching, meaning to restrucutre our data in order to parrallelise computation.\n",
    "\n",
    "- `BATCH_SIZE`: Number of indpenedent sequences will we process in parallel.\n",
    "- `BLOCK_SIZE`: Maximum context length for predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### EXAMPLE BATCH ###\n",
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "BLOCK_SIZE = 8\n",
    "\n",
    "def get_batch(d: torch.tensor):\n",
    "    \"\"\"Generate a batch of data of inputs(x) and target(y).\n",
    "    \"\"\"\n",
    "    # Generate random position to get batch\n",
    "    ix = torch.randint(len(d) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "\n",
    "    x = torch.stack([d[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Create example batch\n",
    "xb, yb = get_batch(train_data)\n",
    "\n",
    "print(f\"\"\"\n",
    "### EXAMPLE BATCH ###\n",
    "Inputs:\n",
    "{xb.shape}\n",
    "{xb}\n",
    "Targets:\n",
    "{yb.shape}\n",
    "{yb}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Neural Networks Time!\n",
    "In this section we will start developing models, based on Neural Networks (NN), and begin passing in our data in batches to see how good we can get our models to be.\n",
    "\n",
    "Our models:\n",
    "1. Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training Loss = 5.036386013031006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training Loss = 2.3838858604431152\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# 1. Bigram Language Model\n",
    "###\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Here we create a lookup matrix where every unique char has a row\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        # Make predictions from our input\n",
    "        logits = self.token_embedding_table(inputs) # (B, T, C)\n",
    "\n",
    "        # Calculate our loss function. cross_entropy implementation\n",
    "        # requires us to reshape our data.\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "###\n",
    "# Test Model\n",
    "###\n",
    "\n",
    "# We want to check if it is able to be initialised and infer.\n",
    "m = BigramModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f\"Before Training Loss = {loss}\")\n",
    "\n",
    "# Here we print out the results from our model with no training.\n",
    "# As we will see the results are very bad since they are totally random.\n",
    "# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "###\n",
    "# Train Model\n",
    "###\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "TRAINING_STEPS = 10_000\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        xb, yb = get_batch(train_data)\n",
    "\n",
    "        # Optimise and calculate Loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f\"After Training Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rulcof tcas\n",
      "S:\n",
      "\n",
      "ar:\n",
      "IV\n",
      "\n",
      "CCARUCiouth mby.\n",
      "\n",
      "ute: tity d Cat LOp; yrer at ifa,\n",
      "BUEzYr: he m, h onubaima\n"
     ]
    }
   ],
   "source": [
    "# Inference after training:\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Our First Self-Attention Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Trick For Computing `x_bow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2 # Batch, Time, Channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# bow: Bag of words\n",
    "#\n",
    "# Here we make an example tensor where we try to implement\n",
    "# a simple self-attention block. In this case an element in\n",
    "# the x_bow tensor stores the mean of all total the elements\n",
    "# which came behind it in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Brute Force For Looping\n",
    "x_bow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b,:t+1] # (t, C)\n",
    "\n",
    "        # Mean over time (0th dim)\n",
    "        x_bow[b, t] = torch.mean(x_prev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Fast Matrix Multiplication\n",
    "# Using triangular lower matricies e.g.\n",
    "# [[1, 0, 0]\n",
    "#  [1, 1, 0]\n",
    "#  [1, 1, 1]]\n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei /  wei.sum(1, keepdim=True)\n",
    "\n",
    "# wei in PyTorch gets treated as (B, T, T)\n",
    "x_bow_mat = wei @ x # (T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check values same\n",
    "torch.allclose(x_bow, x_bow_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Soft Max\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "\n",
    "# Using triangular lower matricies e.g.\n",
    "# [[1, 0, 0]\n",
    "#  [1, 1, 0]\n",
    "#  [1, 1, 1]]\n",
    "# Every index where tril = 0 set the element\n",
    "# at the same index in wei to 'inf'\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "x_bow_sm = wei @ x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check values same\n",
    "torch.allclose(x_bow, x_bow_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6211, -0.3773],\n",
      "        [ 0.0226, -0.7264],\n",
      "        [ 0.5547, -0.6317],\n",
      "        [ 0.4800, -0.6547],\n",
      "        [ 0.2894, -0.3380],\n",
      "        [ 0.5003, -0.3041],\n",
      "        [ 0.1993, -0.1599],\n",
      "        [ 0.1507,  0.0065]])\n"
     ]
    }
   ],
   "source": [
    "pprint(x_bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Crux of The Self-Attention Block\n",
    "We will be implement a small self-attention block for a single head.\n",
    "\n",
    "Here are some notes from Andrej Karpathy on Attention:\n",
    "```\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Single Head\n",
    "HEAD_SIZE = 16\n",
    "key = nn.Linear(C, HEAD_SIZE, bias=False)\n",
    "query = nn.Linear(C, HEAD_SIZE, bias=False)\n",
    "value = nn.Linear(C, HEAD_SIZE, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# Communication between keys and queries\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, T, 16) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v # -> (B, T, HEAD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
