{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a ChatGPT Clone\n",
    "- GPT is a probabilistic system and so can give different answers.\n",
    "- Based on the famous \"Attention Is All You Need\" Transformer model.\n",
    "- G.P.T. stands for Generally Pretrained Transformer.\n",
    "- We will obviously not be building something even close to being as good as OpenAI's ChatGPT (OC).\n",
    "\n",
    "In this case though we are going to be trying to create a model to predict the next character and not the next word (in the case of OC).\n",
    "\n",
    "We will be training on the \"Tiny Shakespeare Dataset\" which is simply a text file containing all the literary works of Shakespeare.\n",
    "\n",
    "We will essentially be modelling how these characters evolve one fter another in a sequence.\n",
    "\n",
    "We will be creating a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b202bb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# RNG seed for reproducability\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str\n",
    "with open('tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read() # Store whole file in 1 string ~ 1 million characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(text)))\n",
    "vocab_size = len(unique_chars)\n",
    "print(''.join(unique_chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google uses Sentence Piece\n",
    "# OpenAI uses Tik Token\n",
    "\n",
    "str_to_int = { ch:i for i, ch in enumerate(unique_chars) }\n",
    "int_to_str = { i:ch for i, ch in enumerate(unique_chars) }\n",
    "\n",
    "# Encoder and Decoder\n",
    "# Here we assign each character an integer. In prod we might\n",
    "# change this to be mutli-character.\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_str[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode tiny-shalespeare.txt dataset into PyTorch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching Our Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLOCK_SIZE = 8\n",
    "train_data[:BLOCK_SIZE+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If INPUT = tensor([18]) ; TARGET = 47\n",
      "If INPUT = tensor([18, 47]) ; TARGET = 56\n",
      "If INPUT = tensor([18, 47, 56]) ; TARGET = 57\n",
      "If INPUT = tensor([18, 47, 56, 57]) ; TARGET = 58\n",
      "If INPUT = tensor([18, 47, 56, 57, 58]) ; TARGET = 1\n",
      "If INPUT = tensor([18, 47, 56, 57, 58,  1]) ; TARGET = 15\n",
      "If INPUT = tensor([18, 47, 56, 57, 58,  1, 15]) ; TARGET = 47\n",
      "If INPUT = tensor([18, 47, 56, 57, 58,  1, 15, 47]) ; TARGET = 58\n"
     ]
    }
   ],
   "source": [
    "X = train_data[:BLOCK_SIZE]\n",
    "Y = train_data[1:BLOCK_SIZE+1]\n",
    "for t in range(BLOCK_SIZE):\n",
    "    context = X[:t+1]\n",
    "    target = Y[t]\n",
    "    print(f\"If INPUT = {context} ; TARGET = {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the information we gaing above about how our model takes a sequence to then learns to predict the next element in that sequence. We split up this task by using a block size which limits the sequence size we are aiming to learn. In the following code we will take this concept of using a block size and also encoporate the idea of batching. Batching, meaning to restrucutre our data in order to parrallelise computation.\n",
    "\n",
    "- `BATCH_SIZE`: Number of indpenedent sequences will we process in parallel.\n",
    "- `BLOCK_SIZE`: Maximum context length for predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### EXAMPLE BATCH ###\n",
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "BLOCK_SIZE = 8\n",
    "\n",
    "def get_batch(d: torch.tensor):\n",
    "    \"\"\"Generate a batch of data of inputs(x) and target(y).\n",
    "    \"\"\"\n",
    "    # Generate random position to get batch\n",
    "    ix = torch.randint(len(d) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "\n",
    "    x = torch.stack([d[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([d[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# Create example batch\n",
    "xb, yb = get_batch(train_data)\n",
    "\n",
    "print(f\"\"\"\n",
    "### EXAMPLE BATCH ###\n",
    "Inputs:\n",
    "{xb.shape}\n",
    "{xb}\n",
    "Targets:\n",
    "{yb.shape}\n",
    "{yb}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Neural Networks Time!\n",
    "In this section we will start developing models, based on Neural Networks (NN), and begin passing in our data in batches to see how good we can get our models to be.\n",
    "\n",
    "Our models:\n",
    "1. Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training Loss = 4.788409233093262\n",
      "After Training Loss = 2.4359238147735596\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# 1. Bigram Language Model\n",
    "###\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Here we create a lookup matrix where every unique char has a row\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        # Make predictions from our input\n",
    "        logits = self.token_embedding_table(inputs) # (B, T, C)\n",
    "\n",
    "        # Calculate our loss function. cross_entropy implementation\n",
    "        # requires us to reshape our data.\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "###\n",
    "# Test Model\n",
    "###\n",
    "\n",
    "# We want to check if it is able to be initialised and infer.\n",
    "m = BigramModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f\"Before Training Loss = {loss}\")\n",
    "\n",
    "# Here we print out the results from our model with no training.\n",
    "# As we will see the results are very bad since they are totally random.\n",
    "# print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "###\n",
    "# Train Model\n",
    "###\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "TRAINING_STEPS = 1_000_000\n",
    "TRAIN = True\n",
    "\n",
    "if TRAIN:\n",
    "    for i in range(TRAINING_STEPS):\n",
    "        xb, yb = get_batch(train_data)\n",
    "\n",
    "        # Optimise and calculate Loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f\"After Training Loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wor th; beno or b, awe cttrthef say,\n",
      "I Grere a my d he heak Dusan ARI u ter wng ustwid, moured'sted \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
